{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Text_Encodings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w4JdGi_Ek3n5",
        "320DN-zQrIYV"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6jU6Y3qkkwU"
      },
      "source": [
        "\n",
        "## LIVE: Text Encodings\n",
        "AppliedAICourse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4JdGi_Ek3n5"
      },
      "source": [
        "### Agenda\n",
        "Prerequsites:\n",
        "- Chapter: \"Real world problem: Predict rating given product reviews on Amazon\".\n",
        "\n",
        "Focus:\n",
        "- Code-walkthrough for each of the methods.\n",
        "- Data Loading\n",
        "- Pre-processing using NLTK.\n",
        "- BoW using SkLearn + Sparse Matrices\n",
        "- TF-IDF using SkLearn\n",
        "- Word2VEc using Gensim\n",
        "- BERT using bert-serving library + APIs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui-G-YK9lY1O"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wULjRbeilXlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac76ba13-9286-4140-f2cc-41bdbfefbc69"
      },
      "source": [
        "# We have seen how to mount GDrive in earlier sessions\n",
        "\n",
        "# This session: download data from internet directly to this box and use it\n",
        "\n",
        "# Source: http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-17 11:22:01--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  19.8MB/s    in 5.3s    \n",
            "\n",
            "2021-12-17 11:22:07 (15.2 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12NKVwXMUMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bb175e-37e2-4391-d4ef-f0f2c2da75fc"
      },
      "source": [
        "# uncompress and see the data\n",
        "! ls\n",
        "! pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb_v1.tar.gz  sample_data\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIBqDuFlOdot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0b09f9-c2e9-4569-9960-464945b2d60e"
      },
      "source": [
        "# it gives an error since the given file format is .tar.gz\n",
        "\n",
        "! unzip aclImdb_v1.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  aclImdb_v1.tar.gz\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of aclImdb_v1.tar.gz or\n",
            "        aclImdb_v1.tar.gz.zip, and cannot find aclImdb_v1.tar.gz.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_OKW4ZKMoJO"
      },
      "source": [
        "#Google: \"Unzip tar gz file colab\" ----> https://stackoverflow.com/questions/49685924/extract-google-drive-zip-from-google-colab-notebook\n",
        "import shutil\n",
        "shutil.unpack_archive(\"/content/aclImdb_v1.tar.gz\", \"/content/\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN3K6p0Rd1Yn",
        "outputId": "8aab9431-22be-41d3-a973-973883e6d846"
      },
      "source": [
        "# it gives the contents in that content folder\n",
        "!ls /content/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb  aclImdb_v1.tar.gz  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YfL_UK1NRBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fa3e03-35d0-4ed1-b3d0-a3e748efe19b"
      },
      "source": [
        "# to see the contents in aclImdb \n",
        "\n",
        "! ls -l /content/aclImdb"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1724\n",
            "-rw-r--r-- 1 7297 1000 903029 Jun 11  2011 imdbEr.txt\n",
            "-rw-r--r-- 1 7297 1000 845980 Apr 12  2011 imdb.vocab\n",
            "-rw-r--r-- 1 7297 1000   4037 Jun 26  2011 README\n",
            "drwxr-xr-x 4 7297 1000   4096 Apr 12  2011 test\n",
            "drwxr-xr-x 5 7297 1000   4096 Jun 26  2011 train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lJFHQLzNhpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3b93c6-073f-4058-ae5e-c3889f732da7"
      },
      "source": [
        "# to see top 10 line in the folder imdb.vocab\n",
        "\n",
        "! head -10 /content/aclImdb/imdb.vocab"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n",
            "and\n",
            "a\n",
            "of\n",
            "to\n",
            "is\n",
            "it\n",
            "in\n",
            "i\n",
            "this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbE3Tr-tNqlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b12f8e-df87-4183-dcab-2491983caf91"
      },
      "source": [
        "# read the description and look the descriptions of al the variables\n",
        "\n",
        "! ls /content/aclImdb/train\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZsjLhLqN6kx"
      },
      "source": [
        "! ls /content/aclImdb/train/pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QscugZfCPTcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff3c95d-10b0-43e8-8dc8-d4202c5a07d0"
      },
      "source": [
        "# we can use cat or head function to see what is in the file\n",
        "\n",
        "! head /content/aclImdb/train/pos/6250_10.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is something special about the Austrian movies not only by Seidl, but by Spielmann and other directors as well. This is the piercing sense of reality that never leaves the viewer throughout the movie. Hundstage is no exception. This effect is achieved not only by the depicted stories but also by actors playing. In Hundstage I have never had the feeling that these are actors playing, but real people instead. So real is the visceral feeling of the viewer...Almost as if the grumpy pensioner or lonely lady in the movie are living below you in your block.<br /><br />Any person living in Vienna can without any doubt painfully recognize the people in the movie with their meckern/sudern (complaining), their hidden sexual urges and the prolo macho guys. This is further reinforced by the Viennese dialect which is, according to many, especially made for complaining as a way of life. A special parochialism and arrogance typical for Vienna are also very well portrayed.<br /><br />The Viennese suburbs have a vivid presence in the movie with their stupor and drowsiness where nothing happens. Moreover, they have been turned into a celebration of materialism with shopping malls and huge department stores. Inbetween are the houses of the people where they indulge into what they reckon is pleasure-giving activities, trying to stay in touch with their human selves, yet in vain. The examples are the sexual game of the old lady with the men which bordered on rape, the prolo guy losing his nerves and hitting his girlfriend and the young woman who hitchhikes and irritates her drivers.<br /><br />The film has no soundtrack as it concentrates on the normality/abnormality of its images only. Another typical feature of Seidl (and other Austrian directors) is his showing of disturbingly sexual images. These include the stripping of the old woman for her husband, the sexual scenes in the bath, the sexual game of the lady with the two men in her apartment, etc.<br /><br />In Hundstage Seild has portrayed the lives of people who eventually may be as much Viennese as they could be citizens of Paris, New York or Madrid. The viewers should not despise or feel pity for the Viennese in the movie as they themselves could become victims of the same human estrangement and alienation, albeit in different circumstances. In the end, I believe Seidl's film is a warning to us about the terrible state of human relationships so brutally revealed in Hundstage. And if the viewer does not succumb to the reasons for this evil transformation, Seidl has achieved his goal."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGH_bManPY5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63dee3f0-6643-4af8-b826-458304e69eb1"
      },
      "source": [
        "# load data from k-reviews from imdb/train/pos reviews\n",
        "# same code as previous sessions [Dimensionality Reduction session]\n",
        "# to append all the individual txt review to a list raw_data[]\n",
        "\n",
        "k=100\n",
        "\n",
        "raw_data = [] # empty list, \n",
        "\n",
        "# Question: Why list of strings? \n",
        "# ans- list accepts duplicates, mutable, indexable, iterable\n",
        "# -most of the libraries accepts 'List' of strings datatype\n",
        "\n",
        "index_file = dict(); # store mapping from index to filename\n",
        "\n",
        "import os\n",
        "directory = r'/content/aclImdb/train/pos/'\n",
        "\n",
        "i=0\n",
        "\n",
        "# os.listdir() lists all the files in the directory\n",
        "for f in os.listdir(directory): # for each file in the subfolder\n",
        "      \n",
        "  if f.endswith(\".txt\"): # check for text file\n",
        "    fname = directory + \"/\" + f\n",
        "    \n",
        "    tmp = open(fname, \"r\") # read file \n",
        "\n",
        "    raw_data.append(tmp.read())\n",
        "    index_file[i] = fname\n",
        "        \n",
        "    i += 1\n",
        "\n",
        "    if i==k: # read k files\n",
        "      break\n",
        "\n",
        "\n",
        "print(i)\n",
        "print(index_file)\n",
        "print(raw_data[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "{0: '/content/aclImdb/train/pos//4406_9.txt', 1: '/content/aclImdb/train/pos//6099_8.txt', 2: '/content/aclImdb/train/pos//7858_10.txt', 3: '/content/aclImdb/train/pos//6088_10.txt', 4: '/content/aclImdb/train/pos//2888_10.txt', 5: '/content/aclImdb/train/pos//3970_7.txt', 6: '/content/aclImdb/train/pos//492_7.txt', 7: '/content/aclImdb/train/pos//2379_8.txt', 8: '/content/aclImdb/train/pos//4843_7.txt', 9: '/content/aclImdb/train/pos//2324_8.txt', 10: '/content/aclImdb/train/pos//2430_10.txt', 11: '/content/aclImdb/train/pos//10132_9.txt', 12: '/content/aclImdb/train/pos//8526_10.txt', 13: '/content/aclImdb/train/pos//4911_9.txt', 14: '/content/aclImdb/train/pos//10312_10.txt', 15: '/content/aclImdb/train/pos//10192_8.txt', 16: '/content/aclImdb/train/pos//197_9.txt', 17: '/content/aclImdb/train/pos//2028_10.txt', 18: '/content/aclImdb/train/pos//6585_10.txt', 19: '/content/aclImdb/train/pos//7959_10.txt', 20: '/content/aclImdb/train/pos//7958_10.txt', 21: '/content/aclImdb/train/pos//6106_9.txt', 22: '/content/aclImdb/train/pos//3524_9.txt', 23: '/content/aclImdb/train/pos//8757_8.txt', 24: '/content/aclImdb/train/pos//3113_9.txt', 25: '/content/aclImdb/train/pos//10232_10.txt', 26: '/content/aclImdb/train/pos//10805_10.txt', 27: '/content/aclImdb/train/pos//6284_7.txt', 28: '/content/aclImdb/train/pos//5000_10.txt', 29: '/content/aclImdb/train/pos//10403_7.txt', 30: '/content/aclImdb/train/pos//7577_8.txt', 31: '/content/aclImdb/train/pos//903_8.txt', 32: '/content/aclImdb/train/pos//592_10.txt', 33: '/content/aclImdb/train/pos//2608_10.txt', 34: '/content/aclImdb/train/pos//12310_10.txt', 35: '/content/aclImdb/train/pos//7087_9.txt', 36: '/content/aclImdb/train/pos//7732_8.txt', 37: '/content/aclImdb/train/pos//6823_10.txt', 38: '/content/aclImdb/train/pos//4586_7.txt', 39: '/content/aclImdb/train/pos//2821_9.txt', 40: '/content/aclImdb/train/pos//6226_10.txt', 41: '/content/aclImdb/train/pos//5832_9.txt', 42: '/content/aclImdb/train/pos//9264_9.txt', 43: '/content/aclImdb/train/pos//9878_7.txt', 44: '/content/aclImdb/train/pos//11109_9.txt', 45: '/content/aclImdb/train/pos//1523_9.txt', 46: '/content/aclImdb/train/pos//7614_10.txt', 47: '/content/aclImdb/train/pos//3025_7.txt', 48: '/content/aclImdb/train/pos//5845_7.txt', 49: '/content/aclImdb/train/pos//9619_10.txt', 50: '/content/aclImdb/train/pos//6036_10.txt', 51: '/content/aclImdb/train/pos//3749_9.txt', 52: '/content/aclImdb/train/pos//6528_9.txt', 53: '/content/aclImdb/train/pos//11954_9.txt', 54: '/content/aclImdb/train/pos//11262_7.txt', 55: '/content/aclImdb/train/pos//1447_8.txt', 56: '/content/aclImdb/train/pos//6413_7.txt', 57: '/content/aclImdb/train/pos//2607_9.txt', 58: '/content/aclImdb/train/pos//729_10.txt', 59: '/content/aclImdb/train/pos//3036_9.txt', 60: '/content/aclImdb/train/pos//9128_9.txt', 61: '/content/aclImdb/train/pos//12122_7.txt', 62: '/content/aclImdb/train/pos//11640_9.txt', 63: '/content/aclImdb/train/pos//7804_10.txt', 64: '/content/aclImdb/train/pos//1052_8.txt', 65: '/content/aclImdb/train/pos//8399_10.txt', 66: '/content/aclImdb/train/pos//11572_8.txt', 67: '/content/aclImdb/train/pos//4830_9.txt', 68: '/content/aclImdb/train/pos//9913_10.txt', 69: '/content/aclImdb/train/pos//10083_7.txt', 70: '/content/aclImdb/train/pos//3273_9.txt', 71: '/content/aclImdb/train/pos//10848_10.txt', 72: '/content/aclImdb/train/pos//8964_8.txt', 73: '/content/aclImdb/train/pos//4936_8.txt', 74: '/content/aclImdb/train/pos//9338_8.txt', 75: '/content/aclImdb/train/pos//7122_8.txt', 76: '/content/aclImdb/train/pos//3282_8.txt', 77: '/content/aclImdb/train/pos//3729_10.txt', 78: '/content/aclImdb/train/pos//5688_8.txt', 79: '/content/aclImdb/train/pos//7075_10.txt', 80: '/content/aclImdb/train/pos//6607_9.txt', 81: '/content/aclImdb/train/pos//11436_9.txt', 82: '/content/aclImdb/train/pos//8165_8.txt', 83: '/content/aclImdb/train/pos//5017_8.txt', 84: '/content/aclImdb/train/pos//4824_7.txt', 85: '/content/aclImdb/train/pos//1661_8.txt', 86: '/content/aclImdb/train/pos//7030_10.txt', 87: '/content/aclImdb/train/pos//8068_9.txt', 88: '/content/aclImdb/train/pos//6388_7.txt', 89: '/content/aclImdb/train/pos//4821_10.txt', 90: '/content/aclImdb/train/pos//6357_9.txt', 91: '/content/aclImdb/train/pos//8209_8.txt', 92: '/content/aclImdb/train/pos//6040_10.txt', 93: '/content/aclImdb/train/pos//10248_7.txt', 94: '/content/aclImdb/train/pos//10938_10.txt', 95: '/content/aclImdb/train/pos//11782_8.txt', 96: '/content/aclImdb/train/pos//10304_7.txt', 97: '/content/aclImdb/train/pos//4627_7.txt', 98: '/content/aclImdb/train/pos//12042_10.txt', 99: '/content/aclImdb/train/pos//9403_8.txt'}\n",
            "Strangely enough this movie never made it to the big screen in Denmark, so I had to wait for the video release. My expectations where high but they where in no way disappointed. As always with Ang Lee there is fantastic acting, an intelligent and thrilling plot that has you guessing right till the end and superb filming. Along with Unforgiven this is easily one of the two best westerns of the 90`s.<br /><br />People who expect something along the line of Mel Gibson in The Patriot(corny) or Braveheart(acceptable) will be sourly disappointed, all others who appreciate the above mentioned qualities will have a fantastic time watching it. 9 out of 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-evMcvglctN"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2tTy116qDgo",
        "outputId": "9975ab42-cf00-47d0-91b2-72ce0e9ab5a9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') # for punctuations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwtfE_-IR0hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2217db4e-ac13-4a64-cf11-ac2e8cf5f581"
      },
      "source": [
        "# 1. remove stop words + tokenize\n",
        "# Google \"stop word removal python\". ----> Many libraries like NLTK, Spacy, Sklearn\n",
        "\n",
        "# https://pythonprogramming.net/stop-words-nltk-tutorial/\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# sent is the file name that we have to give for preprocessing\n",
        "\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "def stopWordRemoval(sent, stop_words):\n",
        "  word_tokens = word_tokenize(sent) # tokenize\n",
        "  filtered_sentence = \"\";\n",
        "  \n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence += ' '+w   # we added space (' ') between words\n",
        "  return filtered_sentence\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # NLTK \n",
        "\n",
        "print(raw_data[0])\n",
        "print(stopWordRemoval(raw_data[0], stop_words) ) # call the function\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a huge baseball fan, my scrutiny of this film is how realistic it appears. Dennis Quaid had all of the right moves and stances of a major league pitcher. It is a fantastic true story told with just a little too much \"Disney\" for my taste.\n",
            " As huge baseball fan , scrutiny film realistic appears . Dennis Quaid right moves stances major league pitcher . It fantastic true story told little much `` Disney '' taste .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYb58jekoqxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01fd9c5-3558-4d38-f835-71f816a8d623"
      },
      "source": [
        "# Lemmatize - uses language specific grammar rules (from plural senese to singular sense)\n",
        "#            - lemmatization has higher time complexity than stemming\n",
        "# stemming - weaker form of lemmatization but faster\n",
        "# note- lemmatization is better than stemming\n",
        "# Source: https://pythonprogramming.net/lemmatizing-nltk-tutorial/\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        " \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"cats\"))\n",
        "print(lemmatizer.lemmatize(\"cacti\"))\n",
        "print(lemmatizer.lemmatize(\"geese\"))\n",
        "print(lemmatizer.lemmatize(\"rocks\"))\n",
        "print(lemmatizer.lemmatize(\"python\"))\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"run\"))\n",
        "print(lemmatizer.lemmatize(\"run\",'v'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "cat\n",
            "cactus\n",
            "goose\n",
            "rock\n",
            "python\n",
            "good\n",
            "best\n",
            "run\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L_rDc7EruaZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGn--Yo2rvbP"
      },
      "source": [
        "# Complete Preprocessing of NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gCe-Izeo0sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8356fb4a-c280-48c6-b774-8fad8183d7f8"
      },
      "source": [
        "# in this cell we are removing stopwords , tokenisation, and lemmatization\n",
        " \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "def stopWord_Lemmatize(sent, stop_words, lemmatizer ):\n",
        "  word_tokens = word_tokenize(sent) # tokenize\n",
        "  return_sent = \"\";\n",
        "  \n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          return_sent += \" \"+ lemmatizer.lemmatize(w)    # lemmatize w beofre adding it to the return_sent\n",
        "  return return_sent\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # NLTK \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(raw_data[0])\n",
        "print(stopWord_Lemmatize(raw_data[0], stop_words, lemmatizer) ) # call the function\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Strangely enough this movie never made it to the big screen in Denmark, so I had to wait for the video release. My expectations where high but they where in no way disappointed. As always with Ang Lee there is fantastic acting, an intelligent and thrilling plot that has you guessing right till the end and superb filming. Along with Unforgiven this is easily one of the two best westerns of the 90`s.<br /><br />People who expect something along the line of Mel Gibson in The Patriot(corny) or Braveheart(acceptable) will be sourly disappointed, all others who appreciate the above mentioned qualities will have a fantastic time watching it. 9 out of 10.\n",
            " Strangely enough movie never made big screen Denmark , I wait video release . My expectation high way disappointed . As always Ang Lee fantastic acting , intelligent thrilling plot guessing right till end superb filming . Along Unforgiven easily one two best western 90`s. < br / > < br / > People expect something along line Mel Gibson The Patriot ( corny ) Braveheart ( acceptable ) sourly disappointed , others appreciate mentioned quality fantastic time watching . 9 10 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THMiq2tppwGb"
      },
      "source": [
        "\n",
        "# pre-process the k sentences and store the result\n",
        "processed_data = []\n",
        "\n",
        "for sent in raw_data:\n",
        "  processed_data.append(stopWord_Lemmatize(sent, stop_words, lemmatizer) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKoSscvZlfPR"
      },
      "source": [
        "### Bag-of-words (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mKbaEcsliiF"
      },
      "source": [
        "# Google \"sklearn bag of words\" ---> https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer() #https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "X_BoW = count_vect.fit_transform(processed_data)\n",
        "\n",
        "# .get_feature_names() gives the feature names \n",
        "\n",
        "print(count_vect.get_feature_names()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv-yEkW9rKI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711c6921-4ce3-4566-84c5-c17d93f3fb24"
      },
      "source": [
        "print(type(X_BoW))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOSNhsyMrSin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9267407-59a4-455d-e325-5fbc0d8f490c"
      },
      "source": [
        "# Sparse representations vs dense Matrices\n",
        "# note - in sparse matrixes, only non zero values will be stored so it reduces space\n",
        "#Refer: https://docs.scipy.org/doc/scipy/reference/sparse.html\n",
        "\n",
        "# .data.nbytes gives the information how many bytes file taken to store\n",
        "print(X_BoW.data.nbytes) # Refer: https://stackoverflow.com/questions/43681279/why-is-scipy-sparse-matrix-memory-usage-indifferent-of-the-number-of-elements-in\n",
        "\n",
        "# Convert X_BoW to dense - convert a sparse matrix to a dense matrix\n",
        "# Ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.todense.html#scipy.sparse.csr_matrix.todense\n",
        "X_BoW_Dense = X_BoW.todense();\n",
        "print(X_BoW_Dense.data.nbytes)\n",
        "\n",
        "# note - we obsersed the sparse matrix taken space 50 times less data than a dense matrix\n",
        "# therefore, it is better to save data in sparse matrix than in dense matrix\n",
        "\n",
        "print(X_BoW.shape)\n",
        "print(X_BoW_Dense.shape)\n",
        "# Always use sparse-matrix in these situations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84152\n",
            "3557600\n",
            "(100, 4447)\n",
            "(100, 4447)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQoWQhgltXl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b90c92d-f5ab-44e7-98d6-f1b7077f7be6"
      },
      "source": [
        "print(X_BoW_Dense[0,:])\n",
        "print(X_BoW_Dense[0,818])\n",
        "print(X_BoW_Dense[0,817])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]]\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfDs-lt6uLP6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtkAMxGAljIq"
      },
      "source": [
        "### TD-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDB1gor2t9yh"
      },
      "source": [
        "# Google: \"TF-IDF SkLearn\" ---> https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "# ngram_range(1,2) - means it will give both 1 gram and 2 grams, (1,1) - only unigrams, (2,2) - only bigrams \n",
        "# ngrams - combination of words - (1gram- 1 word, 2gram- 2 word)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
        "X_tfidf = vectorizer.fit_transform(raw_data)\n",
        "\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nLmIgy2vNjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f778ffe3-d2ea-4051-9c10-5799a898d9e2"
      },
      "source": [
        "print(type(X_tfidf))\n",
        "print(X_tfidf.shape)\n",
        "print(X_tfidf.data.nbytes) # Refer: https://stackoverflow.com/questions/43681279/why-is-scipy-sparse-matrix-memory-usage-indifferent-of-the-number-of-elements-in\n",
        "\n",
        "X_tfidf_dense = X_tfidf.todense()\n",
        "print(type(X_tfidf_dense))\n",
        "print(X_tfidf_dense.shape)\n",
        "print(X_tfidf_dense.data.nbytes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "(100, 14948)\n",
            "158976\n",
            "<class 'numpy.matrix'>\n",
            "(100, 14948)\n",
            "11958400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Eqvyl5lke7"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGNnb80o9YCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdf8292-8a18-46f2-b538-f39a07767dff"
      },
      "source": [
        "# Gensim library\n",
        "# Refer: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "# Download pretrained  vectors\n",
        "# https://www.quora.com/How-can-I-download-the-Google-news-word2vec-pretrained-model-from-a-Ubuntu-terminal\n",
        "! wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "# gunzip is for unzipping .gz files \n",
        "! gunzip GoogleNews-vectors-negative300.bin.gz   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-12 16:31:45--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.29.238\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.29.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  76.4MB/s    in 21s     \n",
            "\n",
            "2021-11-12 16:32:07 (74.5 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMqN_CZD9M0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05cc632b-db17-4e3c-e4f1-b6edc3004aa3"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb  aclImdb_v1.tar.gz  GoogleNews-vectors-negative300.bin\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2OpgG5tDLiR"
      },
      "source": [
        "# takes time as the model is large to laod into RAM\n",
        "# RAM consumption also shoots up. Kernel could restart and give you a larger RAM isnatnce like 25GB RAM\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1uuxybcDWKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8bc2b93-4842-4ff2-8640-59f23293addd"
      },
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('queen', 0.7118192911148071)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPPAs4oGFaWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34f31de-69aa-494d-e1c1-6f3aa209fde2"
      },
      "source": [
        "v = model['queen'];\n",
        "print(type(v))\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKycP4IrlnCg"
      },
      "source": [
        "### BERT\n",
        "\n",
        "- BERT vs Word2Vec\n",
        "- BERT: Contextual encodings\n",
        "- can be used in production but it requires high GPU so it becomes costly\n",
        "- Note- BERT is bi-directional, it gives the values based on meaning of sentence i.e., it give different value for same word.\n",
        "- Note- Bert is much better than w2vec -\n",
        "-Issues- BERT is more computationally expensive but it doesn't take high RAM and w2vec takes high RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgdeT6KK7_Mv"
      },
      "source": [
        "# There are many ways to obatin BERT encodings. We are using one of the \n",
        "# most simple+popular approaches\n",
        "\n",
        "# In DL chpaters, we will see how to do it in Keras.\n",
        "\n",
        "# use GPU based instance: Runtime---> Change Runtime --> GPU\n",
        "\n",
        "# Chaneg to TensorFlow Version 1.x \n",
        "# at present tensorflow version 2 is present but it is not backward compatible meanscode for version1 will not work in version 2\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o10-H_0K8WBG"
      },
      "source": [
        "#https://github.com/hanxiao/bert-as-service - it doesn't work on colab\n",
        "\n",
        "# https://github.com/hanxiao/bert-as-service/issues/380 - works on colab\n",
        "# Install BERT-SERVING client and Server\n",
        "!pip install bert-serving-client\n",
        "!pip install -U bert-serving-server[http]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTHBbZdi8bxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cebf836-c556-446f-fccb-a8574a11d41f"
      },
      "source": [
        "# dowload pretrained models\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "\n",
        "# unzip is for .zip files\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-17 11:36:22--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.128, 142.250.157.128, 142.251.8.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   181MB/s    in 2.1s    \n",
            "\n",
            "2021-12-17 11:36:24 (181 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNJehrNH8fmJ"
      },
      "source": [
        "# Start BERT_SERVER on the current computer\n",
        "!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hgCh2j28pfl"
      },
      "source": [
        "# Use bert-client from python\n",
        "# Takes time to execute as it uses GPU\n",
        "from bert_serving.client import BertClient\n",
        "bc = BertClient()\n",
        "print (bc.encode(['First do it', 'then do it right', 'then do it better'])) # list of setences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpse3FmnA21x"
      },
      "source": [
        "# give data in a list\n",
        "\n",
        "v = bc.encode([raw_data[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mR84fqlnPI03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgPz2p6_Bi0B"
      },
      "source": [
        "print(type(v))\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "320DN-zQrIYV"
      },
      "source": [
        "\n",
        "# To convert .ipynb file to .pdf file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQGjlQ-9rJEC"
      },
      "source": [
        "# first mount the google drive in files option\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWucxdCOx2ZH",
        "outputId": "a8d06fcb-e107-4998-cfbb-5513887a2ea8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RRM4WXLpZnb"
      },
      "source": [
        "%cd drive/My\\Drive/Colab_Notebooks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NLONci3vpulC",
        "outputId": "97f7f3e7-7a70-4471-f521-65aeb41465e2"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab_Notebooks'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE7ENEl_xheO"
      },
      "source": [
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pErSYKYNvfSf"
      },
      "source": [
        "#!jupyter nbconvert --to html pandas_basics_practice.ipynb\n",
        "!jupyter nbconvert -- to pdf NLP_Text_Encodings.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}